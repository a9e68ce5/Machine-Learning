{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk0C23Y/akGE8IpV0VvpIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a9e68ce5/Machine-Learning/blob/main/CNN_to_recognize_handwritten_digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0WAXVBwGT27"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from torch.optim.lr_scheduler import MultiStepLR as MultiStepLR\n",
        "from torch.optim.lr_scheduler import StepLR as StepLR\n",
        "\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "!gdown 1-8MWBqXSnMe5EpVZ7KQMxBKLKDIufM0Y\n",
        "\n",
        "!unzip 'ds2022fallhw6.zip'\n",
        "\n",
        "\"\"\"# ***Set arguments and random seed***\"\"\"\n",
        "\n",
        "TRA_PATH = 'digit/digit/train/'\n",
        "VAL_PATH = 'digit/digit/valid/'\n",
        "TST_PATH = 'digit/digit/test/'\n",
        "LABEL_TRAIN_PATH = 'digit/digit/train.csv'\n",
        "LABEL_VALID_PATH = 'digit/digit/valid.csv'\n",
        "LABEL_SAMPLE_PATH = 'digit/digit/sample.csv'\n",
        "BATCH = 256\n",
        "DEVICE_ID = 0\n",
        "SEED = 5566\n",
        "NUM_ECPOCH = 200\n",
        "\n",
        "torch.cuda.set_device(DEVICE_ID)\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\"\"\"# ***Process data***\"\"\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                 transforms.ToTensor(),\n",
        "                 #transforms.RandomCrop(64,padding=2),#以圖片(PIL Image)中隨機裁減一塊圖像出來\n",
        "                 #transforms.RandomAutocontrast(),\n",
        "                 transforms.RandomVerticalFlip(p=0.5),\n",
        "                 transforms.ToPILImage(mode=None)]   #轉成tensor之後一定要轉回PIL才可以讀\n",
        "                 ) #https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class DataPreparation(Dataset):\n",
        "    def __init__(self,  data_path=None, label_path=None,\n",
        "                 transform=None, target_transform=None):\n",
        "\n",
        "        self.data_path = data_path\n",
        "        self.label_path = label_path\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        ## preprocess files\n",
        "        self.preprocess(self.data_path, self.label_path)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_file = self.data_files[idx]\n",
        "        img_path = os.path.join(self.data_path, data_file)\n",
        "        image = Image.open(img_path) # plt.imread(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.label_path is None:\n",
        "            return image, -1, data_file\n",
        "\n",
        "        label = self.file_labels['label'][self.file_labels['image_name'] == data_file].iloc[0]\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label, data_file\n",
        "\n",
        "    def preprocess(self, data_path, label_path):\n",
        "        self.data_files = os.listdir(data_path)\n",
        "        self.data_files.sort()\n",
        "\n",
        "        if label_path is not None:\n",
        "            self.file_labels = pd.read_csv(label_path)\n",
        "\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, data_path, label_path):\n",
        "\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5), std=(0.5))\n",
        "        ])\n",
        "\n",
        "        # Train Data\n",
        "        train_dataset = DataPreparation(\n",
        "                                        data_path=data_path,\n",
        "                                        label_path=label_path,\n",
        "                                        transform=transform)\n",
        "\n",
        "\n",
        "        self.loader_train = DataLoader(\n",
        "            train_dataset, batch_size=BATCH, shuffle=True,\n",
        "            num_workers=2\n",
        "            )\n",
        "\n",
        "        # Validation Data\n",
        "        valid_data_path = data_path.replace('train', 'valid')\n",
        "        valid_label_path = label_path.replace('train', 'valid')\n",
        "\n",
        "        valid_dataset = DataPreparation(\n",
        "                                       data_path=valid_data_path,\n",
        "                                       label_path=valid_label_path,\n",
        "                                       transform=transform)\n",
        "\n",
        "        self.loader_valid = DataLoader(\n",
        "            valid_dataset, batch_size=BATCH , shuffle=False,\n",
        "            num_workers=2\n",
        "            )\n",
        "\n",
        "        # Test Data (No label, only image)\n",
        "        test_dataset = DataPreparation(\n",
        "                                       data_path=data_path,\n",
        "                                       transform=transform)\n",
        "\n",
        "\n",
        "        self.loader_test = DataLoader(\n",
        "            test_dataset, batch_size=BATCH, shuffle=False,\n",
        "            num_workers=2\n",
        "            )\n",
        "\n",
        "loader = Data( data_path=TRA_PATH , label_path=LABEL_TRAIN_PATH )\n",
        "train_loader = loader.loader_train\n",
        "valid_loader = loader.loader_valid\n",
        "\n",
        "\"\"\"# *** Test Data ***\"\"\"\n",
        "\n",
        "class Data2:\n",
        "    def __init__(self, data_path, label_path):\n",
        "\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5), std=(0.5))\n",
        "        ])\n",
        "\n",
        "        # Test Data\n",
        "        test_dataset = DataPreparation(\n",
        "                                        data_path=data_path,\n",
        "                                        label_path=label_path,\n",
        "                                        transform=transform)\n",
        "\n",
        "\n",
        "        self.loader_test = DataLoader(\n",
        "            test_dataset, batch_size=BATCH, shuffle=False,\n",
        "            num_workers=2\n",
        "            )\n",
        "\n",
        "loader1 = Data2( data_path=TST_PATH , label_path=None)\n",
        "test_loader = loader1.loader_test\n",
        "\n",
        "\"\"\"# ***Define module class***\"\"\"\n",
        "\n",
        "class FaceExpressionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceExpressionNet, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            #torch.nn.Dropout(0.5),\n",
        "            nn.Conv2d(3,32, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(32, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.Sigmoid(),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "            nn.Conv2d(32,64, kernel_size=3, stride=1,padding=1),\n",
        "            nn.BatchNorm2d(64, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "\n",
        "            nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.Sigmoid(),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "\n",
        "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512, eps=1e-05, affine=True),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "\n",
        "            nn.AdaptiveMaxPool2d((1,1)),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512 ,256),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            nn.Linear(256,256),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            nn.Linear(256,256),\n",
        "            nn.LeakyReLU(negative_slope=0.05),\n",
        "            nn.Linear(256,10),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #image size (28,28)\n",
        "        #print(x.shape)\n",
        "        x = self.conv(x) #(14*14)\n",
        "        #print(\"x\",x.shape)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        #print(\"x\",x.shape)\n",
        "        x = self.fc(x)\n",
        "        #print(\"x\",x.shape)\n",
        "        return x\n",
        "\n",
        "\"\"\"# ***Define training and testing process***\"\"\"\n",
        "\n",
        "def train(train_loader, model, loss_fn, use_gpu=True):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    train_result=[]\n",
        "    for (img, label, _) in train_loader:\n",
        "        if use_gpu:\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img)\n",
        "        loss = loss_fn(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():#停止对梯度的计算和存储 ，从而减少对内存的消耗，不会进行反向传播。\n",
        "            predict = torch.argmax(output, dim=-1)#輸出每一行的最大值\n",
        "            acc = np.mean((label == predict).cpu().numpy())\n",
        "            train_acc.append(acc)\n",
        "            train_loss.append(loss.item())\n",
        "            predict=predict.flatten()\n",
        "    train_acc = np.mean(train_acc)\n",
        "    train_loss = np.mean(train_loss)\n",
        "    print(\"Epoch: {}, train Loss: {:.4f}, train Acc: {:.4f}\".format(epoch + 1, train_loss, train_acc))\n",
        "    return train_acc , train_loss\n",
        "def valid(valid_loader, model, loss_fn, use_gpu=True):\n",
        "    model.eval()\n",
        "    with torch.no_grad():#停止对梯度的计算和存储 ，从而减少对内存的消耗，不会进行反向传播。\n",
        "        valid_loss = []\n",
        "        valid_acc = []\n",
        "        for idx, (img, label,_) in enumerate(valid_loader):\n",
        "            if use_gpu:\n",
        "                img = img.to(device)\n",
        "                label = label.to(device)\n",
        "            output = model(img)\n",
        "            loss = loss_fn(output, label)\n",
        "            predict = torch.argmax(output, dim=-1)\n",
        "            acc = (label == predict).cpu().tolist()\n",
        "            valid_loss.append(loss.item())\n",
        "            valid_acc += acc\n",
        "\n",
        "        valid_acc = np.mean(valid_acc)\n",
        "        valid_loss = np.mean(valid_loss)\n",
        "        print(\"Epoch: {}, valid Loss: {:.4f}, valid Acc: {:.4f}\".format(epoch + 1, valid_loss, valid_acc))#{:.4f}:輸出到小數點後四位\n",
        "    return valid_acc , valid_loss\n",
        "\n",
        "def save_checkpoint(valid_acc, acc_record, epoch, prefix='model'):\n",
        "    # you can define the condition to save model :)\n",
        "    if valid_acc >= np.mean(acc_record[-5:]):\n",
        "        checkpoint_path = f'{prefix}.pth'\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print('model saved to %s' % checkpoint_path)\n",
        "    return valid_acc\n",
        "def better(acc_record):\n",
        "    if max(acc_record) == acc_record[-1]: return True\n",
        "    return False\n",
        "\n",
        "\"\"\"# ***實作Early Stopping***\"\"\"\n",
        "\n",
        "class EarlyStopping(object):\n",
        "    def __init__(self, mode='max', min_delta=0, patience=10, percentage=False):\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.num_bad_epochs = 0\n",
        "        self.is_better = None\n",
        "        self._init_is_better(mode, min_delta, percentage)\n",
        "\n",
        "        if patience == 0:\n",
        "            self.is_better = lambda a, b: True #表示暱名函式（ Anonymous Function ）。\n",
        "            self.step = lambda a: False\n",
        "\n",
        "    def step(self, metrics):\n",
        "        if self.best is None:\n",
        "            self.best = metrics\n",
        "            return False\n",
        "\n",
        "        if np.isnan(metrics):#判斷是否是空值\n",
        "            return True\n",
        "\n",
        "        if self.is_better(metrics, self.best):\n",
        "            self.num_bad_epochs = 0\n",
        "            self.best = metrics\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _init_is_better(self, mode, min_delta, percentage):\n",
        "        if mode not in {'min', 'max'}:\n",
        "            raise ValueError('mode ' + mode + ' is unknown!')\n",
        "        if not percentage:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - min_delta\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + min_delta\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - (\n",
        "                            best * min_delta / 100)\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + (\n",
        "                            best * min_delta / 100)\n",
        "\n",
        "\"\"\"# ***Training***\"\"\"\n",
        "\n",
        "es = EarlyStopping(patience = 10)\n",
        "if __name__ == '__main__':\n",
        "    model = FaceExpressionNet()\n",
        "    if use_gpu:\n",
        "        model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = StepLR(optimizer, 20, gamma = 0.1)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_record = []\n",
        "    train_acc = []\n",
        "    train_Loss = []\n",
        "    valid_Loss = []\n",
        "    for epoch in range(NUM_ECPOCH):\n",
        "        scheduler.step(epoch)\n",
        "        result , train_loss = train(train_loader, model, loss_fn, use_gpu)\n",
        "        train_acc.append(result)\n",
        "        valid_acc , valid_loss = valid(valid_loader, model, loss_fn, use_gpu=True)\n",
        "        acc_record.append(valid_acc)\n",
        "        train_Loss.append(train_loss)\n",
        "        valid_Loss.append(valid_loss)\n",
        "        if better(acc_record):\n",
        "            best_acc=save_checkpoint(valid_acc, acc_record, epoch, prefix='model')\n",
        "        if es.step(valid_acc):\n",
        "          print(\"Early stopping with best_acc: {:.4f} at Epoch: {}\". format(best_acc,epoch+1))\n",
        "          break\n",
        "        print('########################################################')\n",
        "\n",
        "\"\"\"# ***cross entropy loss***\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %matplotlib inline\n",
        "fig = plt.figure() #定義一個圖像窗口\n",
        "plt.title(\"CrossEntropy Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "line1,=plt.plot(valid_Loss , '-x',color='red',label='valid learning curve') #定義x,y和圖的樣式\n",
        "line2,=plt.plot(train_Loss, '-x',color='blue',label='training learning curve') #定義x,y和圖的樣式\n",
        "plt.legend(handles = [line1, line2])\n",
        "plt.savefig('/content/CrossEntropy Loss.jpg', bbox_inches='tight')\n",
        "\n",
        "\"\"\"# ***accuracy loss***\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %matplotlib inline\n",
        "fig = plt.figure() #定義一個圖像窗口\n",
        "plt.title(\"valid and training learning curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "line1,=plt.plot(acc_record, '-x',color='red',label='valid learning curve') #定義x,y和圖的樣式\n",
        "line2,=plt.plot(train_acc, '-x',color='blue',label='training learning curve') #定義x,y和圖的樣式\n",
        "plt.legend(handles = [line1, line2])\n",
        "plt.savefig('/content/ with valid and training learning curve.jpg', bbox_inches='tight')\n",
        "\n",
        "\"\"\"# ***Testing***\"\"\"\n",
        "\n",
        "def test2(loader_test, model, output_file_name):\n",
        "    outputs = []\n",
        "    datafiles = []\n",
        "    # switch to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets, datafile) in enumerate(loader_test, 1):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "\n",
        "            preds = model(inputs)\n",
        "\n",
        "            _, output = preds.topk(1, 1, True, True)\n",
        "\n",
        "            outputs.extend(list(output.reshape(-1).cpu().detach().numpy()))\n",
        "\n",
        "            datafiles.extend(list(datafile))\n",
        "\n",
        "\n",
        "\n",
        "    output_file = dict()\n",
        "    output_file['image_name'] = datafiles\n",
        "    output_file['label'] = outputs\n",
        "\n",
        "    output_file = pd.DataFrame.from_dict(output_file)\n",
        "    output_file.to_csv(output_file_name, index = False)\n",
        "\n",
        "del model\n",
        "model = FaceExpressionNet()\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model = model.cuda()\n",
        "test2(test_loader, model , 'predict2.csv')\n",
        "test2(valid_loader, model , 'valid_pred.csv')\n",
        "\n",
        "print(model)\n",
        "\n",
        "\"\"\"# ***confusion matrix and 熱點圖***\"\"\"\n",
        "\n",
        "valid_set=pd.read_csv(LABEL_VALID_PATH)\n",
        "valid_pred = pd.read_csv('/content/valid_pred.csv')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(valid_pred['label'], valid_set['label'])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm,square= True, annot=True, cbar= True)\n",
        "plt.xlabel(\"predicted value\")\n",
        "plt.ylabel(\"true value\")\n",
        "plt.title(\"confusion matrix\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\"# ***Model***\"\"\"\n",
        "\n",
        "!gdown --id \"1t7TlJB6VRhXP1lfx0DCC-nTI2jXNtI7Z\" --output \"model.pth\""
      ]
    }
  ]
}