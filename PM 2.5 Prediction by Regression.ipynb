{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a9e68ce5/Machine-Learning/blob/main/PM%202.5%20Prediction%20by%20Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEu6xnT7jIUx",
        "outputId": "04038ea9-f9da-48ee-a339-88d6a0947860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Hfzrcm69QwdFvdeF0uASoQlcVxKw_hHy\n",
            "To: /content/train.csv\n",
            "100% 324k/324k [00:00<00:00, 109MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FXJztppYG9Q4b_4NHvcmPsc4o5obceWC\n",
            "To: /content/test.csv\n",
            "100% 49.0k/49.0k [00:00<00:00, 58.9MB/s]\n",
            "[[-0.01706186]\n",
            " [-0.01291094]\n",
            " [-0.01097127]\n",
            " [-0.01180289]\n",
            " [-0.00947127]\n",
            " [-0.00930115]\n",
            " [-0.01150716]\n",
            " [-0.01446155]] [[0.7132499 ]\n",
            " [0.49535203]\n",
            " [0.41084807]\n",
            " [0.42030372]\n",
            " [0.40105526]\n",
            " [0.39206728]\n",
            " [0.51342679]\n",
            " [0.79584999]] [1.88097059]\n",
            "(90, 8)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "os.getcwd()\n",
        "import random\n",
        "seed=9487\n",
        "np.random.seed(seed)\n",
        "!gdown --id \"1Hfzrcm69QwdFvdeF0uASoQlcVxKw_hHy\" --output \"train.csv\"\n",
        "!gdown --id '155N6fzI7vAFzHAGdy6jkaWIksWH6Y1G2' --output \"test.csv\"\n",
        "data = pd.read_csv(\"/content/train.csv\")\n",
        "def valid(x, y):\n",
        "  # TODO: Try to filter out extreme values.\n",
        "  #  ex: If PM2.5 > 100, then we don't use the data to train (return False), otherwise return True,\n",
        " #data.corr()['PM2.5'].sort_values()\n",
        " #先將PM2.5的欄位標準化之後若超過正負三倍標準差則剔除\n",
        " mean=data[14].mean()\n",
        " std=data[14].std()\n",
        " y=(data[14]-mean)/std\n",
        " for i in range(5774):\n",
        "   if (y[i]<3 and y[i]>-3):\n",
        "     return True\n",
        "   else:\n",
        "     return False\n",
        "def parse2train(data, feats):\n",
        "#創造2個list,x,y\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  # Use data #0~#7 to predict #8 => Total data length should be decresased by 8.\n",
        "  total_length = data.shape[1] - 8 #data的行數\n",
        "\n",
        "  for i in range(total_length):\n",
        "    x_tmp = data[feats, i:i+8] # Use data #0~#7 to predict #8, data #1~#8 to predict #9, etc.\n",
        "    y_tmp = data[-1, i+8] # last column of (i+8)th row: PM2.5\n",
        "\n",
        "    # Filter out extreme values to train.\n",
        "    if valid(x_tmp, y_tmp):\n",
        "      x.append(x_tmp.reshape(-1,))#自動計算列數\n",
        "      y.append(y_tmp)\n",
        "\n",
        "  # x.shape: (n, 15, 8)\n",
        "  # y.shape: (n, 1)\n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "\n",
        "  return x,y\n",
        "def minibatch(x, y, config):\n",
        "\n",
        "    # Randomize the data in minibatch\n",
        "    index = np.arange(x.shape[0])#x的列數\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    y = y[index]\n",
        "\n",
        "    # Initialization\n",
        "    batch_size = config.batch_size\n",
        "    lr = config.lr #learning rate\n",
        "    lam = config.lam\n",
        "    epoch = config.epoch\n",
        "\n",
        "    beta_1 = np.full(x[0].shape, 0.9).reshape(-1, 1)#建立以x的列數為大小且都是0.9的n*1陣列\n",
        "    beta_2 = np.full(x[0].shape, 0.99).reshape(-1, 1) #建立以x的列數為大小且都是0.99的n*1陣列\n",
        "    # Linear regression: only contains two parameters (w, b).\n",
        "    #Implement 2-nd polynomial regression version for the report.\n",
        "    #y=w_1X^2+w_2X+b\n",
        "    w_1 = np.full(x[0].shape, 0.1).reshape(-1, 1) #建立以x的列數為大小且都是0.1的n*1陣列\n",
        "    w_2 = np.full(x[0].shape, 0.1).reshape(-1, 1) #建立以x的列數為大小且都是0.1的n*1陣列\n",
        "    bias = 0.1\n",
        "    m_t_1 = np.full(x[0].shape, 0).reshape(-1, 1) #建立以x的列數為大小且都是0的n*1陣列\n",
        "    m_t_2 = np.full(x[0].shape, 0).reshape(-1, 1) #建立以x的列數為大小且都是0的n*1陣列\n",
        "    v_t_1 = np.full(x[0].shape, 0).reshape(-1, 1) #建立以x的列數為大小且都是0的n*1陣列\n",
        "    v_t_2 = np.full(x[0].shape, 0).reshape(-1, 1) #建立以x的列數為大小且都是0的n*1陣列\n",
        "    m_t_b = 0.0 #b的momentum\n",
        "    v_t_b = 0.0 #v的momentum\n",
        "    t = 0\n",
        "    epsilon = 1e-8 #10^-8\n",
        "\n",
        "    # Training loop\n",
        "    for num in range(epoch):\n",
        "        for b in range(int(x.shape[0]/batch_size)):\n",
        "            t+=1\n",
        "            x_batch = x[b*batch_size:(b+1)*batch_size]\n",
        "            y_batch = y[b*batch_size:(b+1)*batch_size].reshape(-1,1)\n",
        "            x_x=x_batch*x_batch\n",
        "            # Prediction of linear regression\n",
        "            pred = np.dot(x_x,w_1)+np.dot(x_batch,w_2) + bias #y=w_1X^2+w_2X+b\n",
        "            # loss\n",
        "            loss = y_batch - pred #y-(w_1X^2+w_2X+b)\n",
        "\n",
        "            # Compute gradient\n",
        "\n",
        "            g_t_1 = np.dot(x_x.transpose(),loss) * (-2) +  2 * lam * np.sum(w_1)\n",
        "            g_t_2 = np.dot(x_batch.transpose(),loss) * (-2) +  2 * lam * np.sum(w_2)\n",
        "            g_t_b = loss.sum(axis=0) * (-2)\n",
        "            m_t_1 = beta_1*m_t_1 + (1-beta_1)*g_t_1\n",
        "            m_t_2 = beta_1*m_t_2 + (1-beta_1)*g_t_2\n",
        "            v_t_1 = beta_2*v_t_1 + (1-beta_2)*np.multiply(g_t_1, g_t_1)\n",
        "            v_t_2 = beta_2*v_t_2 + (1-beta_2)*np.multiply(g_t_2, g_t_2)\n",
        "            m_cap_1 = m_t_1/(1-(beta_1**t))\n",
        "            m_cap_2 = m_t_2/(1-(beta_1**t))\n",
        "            v_cap_1 = v_t_1/(1-(beta_2**t))\n",
        "            v_cap_2 = v_t_2/(1-(beta_2**t))\n",
        "            m_t_b = 0.9*m_t_b + (1-0.9)*g_t_b\n",
        "            v_t_b = 0.99*v_t_b + (1-0.99)*(g_t_b*g_t_b)\n",
        "            m_cap_b = m_t_b/(1-(0.9**t))\n",
        "            v_cap_b = v_t_b/(1-(0.99**t))\n",
        "\n",
        "\n",
        "            # Update weight & bias\n",
        "            w_1 -= ((lr*m_cap_1)/(np.sqrt(v_cap_1)+epsilon)).reshape(-1, 1)\n",
        "            w_2 -= ((lr*m_cap_2)/(np.sqrt(v_cap_2)+epsilon)).reshape(-1, 1)\n",
        "            bias -= (lr*m_cap_b)/(math.sqrt(v_cap_b)+epsilon)\n",
        "\n",
        "\n",
        "    return w_1,w_2, bias\n",
        "from argparse import Namespace\n",
        "\n",
        "# TODO: Tune the config to boost your performance.\n",
        "train_config = Namespace(\n",
        "    batch_size = 200,\n",
        "    lr = 0.00000348,\n",
        "    lam = 0.1,\n",
        "    epoch = 20000,\n",
        ")\n",
        "feats = [1]\n",
        "data = data.values\n",
        "train_data = np.transpose(np.array(np.float64(data)))\n",
        "train_x,train_y = parse2train(train_data,feats)\n",
        "w_1,w_2, bias = minibatch(train_x, train_y,train_config)\n",
        "data = pd.read_csv('test.csv')\n",
        "data = data.values\n",
        "def parse2test(data, feats):\n",
        "  x = []\n",
        "  for i in range(90):\n",
        "    x_tmp = data[feats,8*i: 8*i+8]\n",
        "    x.append(x_tmp.reshape(-1,))\n",
        "\n",
        "  # x.shape: (n, 15, 8)\n",
        "  x = np.array(x)\n",
        "  return x\n",
        "print(w_1,w_2,bias)\n",
        "test_data = np.transpose(np.array(np.float64(data)))\n",
        "test_x = parse2test(test_data, feats)\n",
        "with open('my_sol.csv', 'w', newline='') as csvf:\n",
        "    # 建立 CSV 檔寫入器\n",
        "    writer = csv.writer(csvf)\n",
        "    writer.writerow(['Id','Predicted'])\n",
        "\n",
        "    print(test_x.shape)\n",
        "    for i in range(int(test_x.shape[0])):\n",
        "      # Prediction of linear regression\n",
        "      x_x=test_x[i]**2\n",
        "      prediction = (np.dot(np.reshape(w_1,-1),x_x)+np.dot(np.reshape(w_2,-1),test_x[i]) + bias)[0]\n",
        "      writer.writerow([i, prediction] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvl_NHuPAy7U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}